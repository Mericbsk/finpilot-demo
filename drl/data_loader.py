"""
Data loader and feature engineering module for FinPilot.
Handles fetching real market data and calculating technical indicators
matching the DRL agent's expectations.
"""

import logging
import re
from datetime import datetime, timedelta
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import yfinance as yf

from drl.config import DEFAULT_CONFIG, MarketEnvConfig
from drl.rate_limiter import exponential_backoff, yfinance_rate_limit

if TYPE_CHECKING:
    from drl.market_env import EpisodeData

# Logger yapılandırması
logger = logging.getLogger(__name__)

# Güvenli sembol regex'i (sadece harf, rakam, tire ve nokta)
VALID_SYMBOL_PATTERN = re.compile(r"^[A-Za-z0-9\.\-]{1,20}$")


def validate_symbol(symbol: str) -> str:
    """
    Sembol parametresini doğrular ve sanitize eder.
    Geçersiz semboller için ValueError fırlatır.
    """
    if not symbol or not isinstance(symbol, str):
        raise ValueError("Sembol boş veya geçersiz tip")

    symbol = symbol.strip().upper()

    if not VALID_SYMBOL_PATTERN.match(symbol):
        raise ValueError(f"Geçersiz sembol formatı: {symbol}")

    return symbol


def calculate_technical_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculates technical indicators required by the FeaturePipeline.
    Expects df to have 'Close', 'High', 'Low', 'Volume' columns.
    """
    df = df.copy()

    # Ensure lowercase columns for consistency with DRL config
    df.columns = [c.lower() for c in df.columns]

    # Basic EMAs
    df["ema_20"] = df["close"].ewm(span=20, adjust=False).mean()
    df["ema_50"] = df["close"].ewm(span=50, adjust=False).mean()
    df["ema_200"] = df["close"].ewm(span=200, adjust=False).mean()

    # RSI (14)
    delta = df["close"].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df["rsi"] = 100 - (100 / (1 + rs))

    # MACD
    ema_12 = df["close"].ewm(span=12, adjust=False).mean()
    ema_26 = df["close"].ewm(span=26, adjust=False).mean()
    df["macd"] = ema_12 - ema_26
    df["macd_signal"] = df["macd"].ewm(span=9, adjust=False).mean()
    df["macd_hist"] = df["macd"] - df["macd_signal"]

    # Bollinger Bands (20, 2)
    rolling_mean = df["close"].rolling(window=20).mean()
    rolling_std = df["close"].rolling(window=20).std()
    df["bb_upper"] = rolling_mean + (2 * rolling_std)
    df["bb_lower"] = rolling_mean - (2 * rolling_std)

    # ATR (14)
    high_low = df["high"] - df["low"]
    high_close = np.abs(df["high"] - df["close"].shift())
    low_close = np.abs(df["low"] - df["close"].shift())
    ranges = pd.concat([pd.Series(high_low), pd.Series(high_close), pd.Series(low_close)], axis=1)
    true_range = ranges.max(axis=1)
    df["atr"] = true_range.rolling(14).mean()

    # Volume Average
    df["volume_avg_20"] = df["volume"].rolling(20).mean()

    # Returns
    df["returns"] = df["close"].pct_change()
    df["log_returns"] = np.log(df["close"] / df["close"].shift(1))

    # Regime Identification (Simple Rule-based for now)
    # Volatility Regime: ATR / Close > 0.02 (2% daily range is volatile)
    # Trend Regime: Price > EMA50 > EMA200
    df["volatility_ratio"] = df["atr"] / df["close"]

    conditions = [
        (df["volatility_ratio"] > 0.02),
        (df["close"] > df["ema_50"]) & (df["ema_50"] > df["ema_200"]),
    ]
    choices = ["volatility", "trend"]
    df["regime"] = np.select(conditions, choices, default="range")

    df["regime_trend"] = (df["regime"] == "trend").astype(float)
    df["regime_range"] = (df["regime"] == "range").astype(float)
    df["regime_volatility"] = (df["regime"] == "volatility").astype(float)

    # Fill NaN values generated by rolling windows
    df = df.bfill().ffill()

    return df


@yfinance_rate_limit
@exponential_backoff(max_retries=2, base_delay=1.0, exceptions=(Exception,))
def fetch_market_data(symbol: str, period: str = "6mo", interval: str = "1h") -> pd.DataFrame:
    """
    Fetches real market data from YFinance and calculates features.

    Args:
        symbol: Hisse senedi sembolü (örn. "AAPL", "BTC-USD")
        period: Veri dönemi (örn. "6mo", "1y")
        interval: Zaman aralığı (örn. "1h", "1d")

    Returns:
        Teknik indikatörlerle zenginleştirilmiş DataFrame

    Raises:
        ValueError: Geçersiz sembol formatı
    """
    # Girdi doğrulama
    symbol = validate_symbol(symbol)

    logger.info(f"Fetching data for {symbol}...")
    try:
        df = yf.download(symbol, period=period, interval=interval, progress=False, ignore_tz=True)
        if df is None or df.empty:
            logger.warning(f"No data found for {symbol}")
            raise ValueError(f"No data found for {symbol}")

        # Handle MultiIndex columns if present (yfinance update)
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)

        return calculate_technical_features(df)

    except ValueError:
        raise  # Doğrulama hatalarını tekrar fırlat
    except Exception as e:
        logger.error(f"Error fetching data for {symbol}: {e}")
        return pd.DataFrame()


def prepare_inference_frame(symbol: str, config: MarketEnvConfig) -> pd.DataFrame:
    """
    Prepares a dataframe ready for the FeaturePipeline.
    Merges technical data with AltData.
    """
    from altdata import get_altdata_history
    from drl.config import MarketEnvConfig

    # 1. Get Price Data
    df = fetch_market_data(symbol)
    if df.empty:
        return pd.DataFrame()

    # 2. Get Alt Data (uses real data proxy now)
    # We ask for a bit more history to cover the tail
    alt_df = get_altdata_history(symbol, periods=len(df), freq="1h")

    # Align indices (AltData might be slightly off due to generation method, so we merge/join)
    # For now, simplistic assignment as lengths mimic each other in current altdata impl
    # But altdata returns tail, so let's match tail
    tgt_len = min(len(df), len(alt_df))
    df = df.iloc[-tgt_len:].copy()
    alt_df = alt_df.iloc[-tgt_len:].copy()

    df["sentiment_score"] = alt_df["sentiment_score"].values
    df["onchain_tx_volume"] = alt_df["onchain_tx_volume"].values

    # 3. Add Placeholder Agent State features (Required by Config)
    # Since this is "Inference" before we open a trade, we assume neutral state
    df["cash_ratio"] = 1.0
    df["position_ratio"] = 0.0
    df["open_risk"] = 0.0
    df["kelly_fraction"] = 0.0

    # 4. Ensure all columns exist
    missing = [c for c in config.feature_columns if c not in df.columns]
    for c in missing:
        df[c] = 0.0

    return df


# ============================================================================
# TRAINING DATA FUNCTIONS
# ============================================================================


def fetch_training_data(
    symbols: List[str],
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    period: str = "2y",
    interval: str = "1d",
) -> Dict[str, pd.DataFrame]:
    """
    Çoklu sembol için training verisi çeker.

    Args:
        symbols: Sembol listesi (örn. ["AAPL", "MSFT", "NVDA"])
        start_date: Başlangıç tarihi (YYYY-MM-DD)
        end_date: Bitiş tarihi (YYYY-MM-DD)
        period: yfinance period (start_date yoksa)
        interval: Zaman aralığı ("1d", "1h", "15m")

    Returns:
        Dict[symbol, DataFrame] - Her sembol için feature DataFrame
    """
    results = {}

    for symbol in symbols:
        try:
            symbol = validate_symbol(symbol)
            logger.info(f"Fetching training data for {symbol}...")

            if start_date and end_date:
                df = yf.download(
                    symbol,
                    start=start_date,
                    end=end_date,
                    interval=interval,
                    progress=False,
                    ignore_tz=True,
                )
            else:
                df = yf.download(
                    symbol, period=period, interval=interval, progress=False, ignore_tz=True
                )

            if df is None or df.empty:
                logger.warning(f"No data found for {symbol}")
                continue

            # Handle MultiIndex columns
            if isinstance(df.columns, pd.MultiIndex):
                df.columns = df.columns.get_level_values(0)

            # Calculate features
            df = calculate_technical_features(df)

            # Add placeholder columns for missing features
            df = _add_placeholder_features(df)

            results[symbol] = df
            logger.info(f"✓ {symbol}: {len(df)} rows loaded")

        except Exception as e:
            logger.error(f"Error fetching {symbol}: {e}")
            continue

    return results


def _add_placeholder_features(df: pd.DataFrame) -> pd.DataFrame:
    """Eksik feature'ları placeholder değerlerle doldurur."""
    df = df.copy()

    # Sentiment features (eğer yoksa)
    if "sentiment_score" not in df.columns:
        df["sentiment_score"] = 0.0
    if "news_sentiment" not in df.columns:
        df["news_sentiment"] = 0.0

    # On-chain features
    if "onchain_active_addresses" not in df.columns:
        df["onchain_active_addresses"] = 0.0
    if "onchain_tx_volume" not in df.columns:
        df["onchain_tx_volume"] = 0.0

    # Portfolio state (training için initial state)
    if "cash_ratio" not in df.columns:
        df["cash_ratio"] = 1.0
    if "position_ratio" not in df.columns:
        df["position_ratio"] = 0.0
    if "open_risk" not in df.columns:
        df["open_risk"] = 0.0
    if "kelly_fraction" not in df.columns:
        df["kelly_fraction"] = 0.0

    return df


def create_train_test_split(
    df: pd.DataFrame, train_ratio: float = 0.8, validation_ratio: float = 0.0
) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:
    """
    Kronolojik train/test split yapar (shuffle yok!).

    Args:
        df: Feature DataFrame
        train_ratio: Training oranı (default 0.8)
        validation_ratio: Validation oranı (default 0, yani yok)

    Returns:
        (train_df, test_df, validation_df veya None)
    """
    n = len(df)
    train_end = int(n * train_ratio)

    if validation_ratio > 0:
        val_end = train_end + int(n * validation_ratio)
        train_df = df.iloc[:train_end].copy()
        val_df = df.iloc[train_end:val_end].copy()
        test_df = df.iloc[val_end:].copy()
        return train_df, test_df, val_df
    else:
        train_df = df.iloc[:train_end].copy()
        test_df = df.iloc[train_end:].copy()
        return train_df, test_df, None


def create_walk_forward_splits(
    df: pd.DataFrame,
    n_splits: int = 5,
    train_window: Optional[int] = None,
    test_window: Optional[int] = None,
    gap: int = 0,
) -> List[Tuple[pd.DataFrame, pd.DataFrame, str]]:
    """
    Walk-forward optimization için veri bölümleri oluşturur.

    Walk-forward, her pencerede modeli eğitip bir sonraki dönemde
    test ederek overfitting'i azaltır.

    Args:
        df: Feature DataFrame
        n_splits: Kaç adet split oluşturulacak
        train_window: Training pencere boyutu (None = otomatik)
        test_window: Test pencere boyutu (None = otomatik)
        gap: Train ve test arası boşluk (lookahead bias önleme)

    Returns:
        List of (train_df, test_df, label) tuples

    Example:
        >>> splits = create_walk_forward_splits(df, n_splits=5)
        >>> for train, test, label in splits:
        ...     model.fit(train)
        ...     score = model.evaluate(test)
    """
    n = len(df)

    if train_window is None:
        # Her split için yaklaşık %60 train
        train_window = int(n * 0.6 / n_splits) * n_splits

    if test_window is None:
        # Her split için yaklaşık %20 test
        test_window = max(20, int((n - train_window - gap * n_splits) / n_splits))

    splits = []
    step_size = test_window

    for i in range(n_splits):
        test_start = train_window + gap + (i * step_size)
        test_end = test_start + test_window

        if test_end > n:
            break

        train_start = max(0, test_start - train_window - gap)
        train_end = test_start - gap

        train_df = df.iloc[train_start:train_end].copy()
        test_df = df.iloc[test_start:test_end].copy()

        label = f"split_{i+1}_train{train_start}-{train_end}_test{test_start}-{test_end}"
        splits.append((train_df, test_df, label))

        logger.info(
            f"Split {i+1}: Train[{train_start}:{train_end}] ({len(train_df)} rows), "
            f"Test[{test_start}:{test_end}] ({len(test_df)} rows)"
        )

    return splits


def merge_multi_symbol_data(
    data_dict: Dict[str, pd.DataFrame], method: str = "concat"
) -> pd.DataFrame:
    """
    Çoklu sembol verisini birleştirir.

    Args:
        data_dict: {symbol: DataFrame} dictionary
        method:
            - "concat": Tüm sembolleri alt alta birleştir
            - "panel": Sembolleri yan yana birleştir (çok sütunlu)

    Returns:
        Birleştirilmiş DataFrame
    """
    if not data_dict:
        return pd.DataFrame()

    if method == "concat":
        dfs = []
        for symbol, df in data_dict.items():
            df_copy = df.copy()
            df_copy["symbol"] = symbol
            dfs.append(df_copy)
        return pd.concat(dfs, ignore_index=False)

    elif method == "panel":
        # Multi-index panel data
        panels = {}
        for symbol, df in data_dict.items():
            panels[symbol] = df
        return pd.concat(panels, axis=1, keys=data_dict.keys())

    else:
        raise ValueError(f"Unknown method: {method}")


def get_default_training_symbols() -> List[str]:
    """Training için varsayılan sembol listesi."""
    return [
        # US Large Cap
        "AAPL",
        "MSFT",
        "GOOGL",
        "AMZN",
        "NVDA",
        "META",
        "TSLA",
        # US Tech
        "AMD",
        "CRM",
        "ADBE",
        "INTC",
        # ETFs
        "SPY",
        "QQQ",
        "IWM",
    ]


def prepare_episode_data(
    df: pd.DataFrame, config: Optional[MarketEnvConfig] = None
) -> "EpisodeData":
    """
    DataFrame'i MarketEnv için EpisodeData formatına dönüştürür.

    Args:
        df: Feature DataFrame
        config: MarketEnvConfig (default kullanılır)

    Returns:
        EpisodeData instance
    """
    from drl.feature_pipeline import FeatureFrame
    from drl.market_env import EpisodeData

    if config is None:
        config = DEFAULT_CONFIG

    # Ensure all required columns exist
    df = _add_placeholder_features(df)

    # Validate columns
    missing = [c for c in config.feature_columns if c not in df.columns]
    for c in missing:
        df[c] = 0.0
        logger.warning(f"Missing feature column '{c}', filled with 0.0")

    # Create FeatureFrame
    features = FeatureFrame(data=df)

    # Extract prices and regime
    prices = df["close"] if "close" in df.columns else df.iloc[:, 0]

    regimes = None
    if "regime" in df.columns:
        regimes = df["regime"]

    return EpisodeData(
        features=features,
        prices=prices,
        regimes=regimes,
        timestamps=df.index if isinstance(df.index, pd.DatetimeIndex) else None,
    )


# ============================================================================
# CONVENIENCE FUNCTIONS
# ============================================================================


def quick_load_training_data(
    symbols: Optional[List[str]] = None, period: str = "1y", interval: str = "1d"
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Hızlı training verisi yükleme (tek satırda kullanım).

    Args:
        symbols: Sembol listesi (None = defaults)
        period: Veri dönemi
        interval: Zaman aralığı

    Returns:
        (train_df, test_df) tuple
    """
    if symbols is None:
        symbols = get_default_training_symbols()[:5]  # İlk 5 sembol

    data = fetch_training_data(symbols, period=period, interval=interval)
    merged = merge_multi_symbol_data(data, method="concat")

    if merged.empty:
        raise ValueError("No data could be fetched for training")

    train_df, test_df, _ = create_train_test_split(merged, train_ratio=0.8)
    return train_df, test_df
